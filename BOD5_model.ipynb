{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLdNAOkn_xI6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uA4vvSx5AVWx"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujVXAhHu_iYr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import random\n",
        "import regex as re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oDp5PWWAYVn"
      },
      "outputs": [],
      "source": [
        "def extract_data(zip_folders):\n",
        "  \"\"\"\n",
        "  Extracts images and labels from zip files\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  image_labels = []\n",
        "  for zip_folder in zip_folders:\n",
        "    with zipfile.ZipFile(zip_folder + '.zip', 'r') as zip_ref:\n",
        "      filenames = zip_ref.namelist()\n",
        "\n",
        "      for filename in filenames:\n",
        "        images.append(filename)\n",
        "        # Get BOD5 value from filename\n",
        "        filename = os.path.splitext(filename)\n",
        "        tokens = re.split('_', filename[0])\n",
        "        value = float(tokens[-1])\n",
        "        image_labels.append(value)\n",
        "\n",
        "  return images, image_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCaHQqsymJFb"
      },
      "outputs": [],
      "source": [
        "train_zips = [\"/content/drive/MyDrive/Training_Set_1\", \"/content/drive/MyDrive/Training_Set_2\"\n",
        ",\"/content/drive/MyDrive/Training_Set_3\", \"/content/drive/MyDrive/Training_Set_4\"\n",
        ",\"/content/drive/MyDrive/Training_Set_5\", \"/content/drive/MyDrive/Training_Set_6\"\n",
        ",\"/content/drive/MyDrive/Training_Set_7\"]\n",
        "\n",
        "val_zip = [\"/content/drive/MyDrive/Validation_Set\"]\n",
        "\n",
        "train_images, train_labels = extract_data(train_zips)\n",
        "val_images, val_labels = extract_data(val_zip)\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "val_images = np.array(val_images)\n",
        "val_labels = np.array(val_labels)\n",
        "print(val_images.shape)\n",
        "print(val_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK8LojiEB-ll"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import rasterio\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && apt install cuda-11-8\n",
        "pip install deeplabcut[tf]"
      ],
      "metadata": {
        "id": "8swmg0ISuwv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6rSLi1_9qHz"
      },
      "outputs": [],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found - On for CPU time!')\n",
        "else:\n",
        "    print('Found GPU at {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCPJQo6mJ5EL"
      },
      "outputs": [],
      "source": [
        "def extract_file(zip_folder, image_names):\n",
        "  \"\"\"\n",
        "  Extracts and open images into local Colab Directory\n",
        "  \"\"\"\n",
        "  batch_images = []\n",
        "  with zipfile.ZipFile(zip_folder + '.zip', 'r') as zip_ref:\n",
        "    for image_name in image_names:\n",
        "      zip_ref.extract(image_name, '/content/temp_dataset')\n",
        "      image_path = os.path.join('/content/temp_dataset', image_name)\n",
        "      with rasterio.open(image_path, 'r') as src:\n",
        "        image = src.read()\n",
        "        image = np.transpose(image, (1,2,0))\n",
        "        image = cv2.resize(image, (512,512))\n",
        "        image[np.isnan(image)] = 0\n",
        "        batch_images.append(image)\n",
        "\n",
        "    shutil.rmtree('/content/temp_dataset')\n",
        "    return batch_images\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "  def __init__(self, image_names, labels, zip_folders, batch_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.image_names = image_names\n",
        "    self.labels = labels\n",
        "    self.zip_folders = zip_folders\n",
        "    self.batch_size = batch_size\n",
        "    self.folder_idx = 0;\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_names) // self.batch_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    image_sublist = self.image_names[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "    if len(self.zip_folders) > 1:\n",
        "        folder_idx = (idx*self.batch_size) // 1024\n",
        "    else:\n",
        "        folder_idx = 0\n",
        "\n",
        "    batch_images = extract_file(self.zip_folders[folder_idx], image_sublist)\n",
        "    batch_labels = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "    np_images = np.array(batch_images)\n",
        "    np_labels = np.array(batch_labels)\n",
        "\n",
        "    return np_images, np_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgaYmaFkCxJl"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataset = DataGenerator(train_images, train_labels, train_zips, batch_size)\n",
        "val_dataset = DataGenerator(val_images, val_labels, val_zip, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoywo2djTBw-"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(644,644,12)),\n",
        "    tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.Conv2D(24, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((3, 3)),\n",
        "    tf.keras.layers.Conv2D(48, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((3, 3)),\n",
        "    tf.keras.layers.Conv2D(96, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.Conv2D(108, (3, 3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((3, 3)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1, activation=None)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5PZ01ZYC1F4"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1WT498pmC30u"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h2lyY2soEDiH"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=[tf.keras.metrics.R2Score()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BBdBhD52EZ3t"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/model_checkpoints', exist_ok=True)\n",
        "save_path = os.path.join('/content/drive/MyDrive/model_checkpoints', 'model_aug.keras')\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(save_path,\n",
        "                                                      monitor='val_loss',\n",
        "                                                      verbose=0,\n",
        "                                                      save_best_only=False,\n",
        "                                                      save_weights_only=False,\n",
        "                                                      save_freq='epoch')\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=val_dataset,\n",
        "                    epochs=100,\n",
        "                    batch_size=batch_size,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping, lr_reduce, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEWjfWYHEarv"
      },
      "outputs": [],
      "source": [
        "def training_plot(metrics, history):\n",
        "    f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax[idx].plot(history.history[metric], ls='dashed')\n",
        "        ax[idx].set_xlabel('Epochs')\n",
        "        ax[idx].set_ylabel(metric)\n",
        "        ax[idx].plot(history.history['val_'+metric]);\n",
        "        ax[idx].legend(['train_'+metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8xXOP4JGGYw"
      },
      "outputs": [],
      "source": [
        "training_plot(['loss', 'r2_score'], history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkJtXtheCPI8"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/model_weights', exist_ok=True)\n",
        "path = os.path.join('/content/drive/MyDrive/model_weights', 'model_weights.h5')\n",
        "model.save_weights(path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
